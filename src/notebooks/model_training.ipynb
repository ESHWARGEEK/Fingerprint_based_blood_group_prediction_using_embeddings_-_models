{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313b0c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Config\n",
    "data_dir = \"/kaggle/working/dataset_split\"\n",
    "num_classes = 8\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])  # ImageNet norm\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(os.path.join(data_dir,\"train\"), transform=transform)\n",
    "val_ds   = datasets.ImageFolder(os.path.join(data_dir,\"val\"), transform=transform)\n",
    "test_ds  = datasets.ImageFolder(os.path.join(data_dir,\"test\"), transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.classes\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "\n",
    "# ----- Training function with early stopping -----\n",
    "def train_model(model, name, head_epochs=5, finetune_epochs=20, patience=5, lr_head=1e-3, lr_finetune=1e-5):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    no_improve = 0\n",
    "\n",
    "    # ---- Phase A: Train classifier head ----\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc.parameters() if hasattr(model, 'fc') else model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr_head, weight_decay=1e-4)\n",
    "\n",
    "    print(f\"\\n[{name}] Phase A: Training head for {head_epochs} epochs\")\n",
    "    for epoch in range(head_epochs):\n",
    "        train_one_epoch(model, optimizer, criterion, train_loader, val_loader, epoch, head_epochs, name)\n",
    "    \n",
    "    # ---- Phase B: Fine-tune backbone ----\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr_finetune, weight_decay=1e-4)\n",
    "\n",
    "    print(f\"\\n[{name}] Phase B: Fine-tuning backbone for {finetune_epochs} epochs\")\n",
    "    for epoch in range(finetune_epochs):\n",
    "        train_loss, train_acc, val_acc = train_one_epoch(model, optimizer, criterion, train_loader, val_loader, epoch, finetune_epochs, name)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"[{name}] Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Load best weights and save\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), f\"{name}_best.pth\")\n",
    "    print(f\"[{name}] Best Val Acc: {best_acc:.2f}%\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, criterion, train_loader, val_loader, epoch, total_epochs, name):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    # Validation accuracy\n",
    "    model.eval()\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "    val_acc = 100. * correct_val / total_val\n",
    "\n",
    "    print(f\"[{name}] Epoch {epoch+1}/{total_epochs} \"\n",
    "          f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return running_loss/len(train_loader), train_acc, val_acc\n",
    "\n",
    "\n",
    "# ---- Train Models ----\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "resnet = train_model(resnet, \"ResNet50\", head_epochs=8, finetune_epochs=25)\n",
    "\n",
    "eff = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "eff.classifier[1] = nn.Linear(eff.classifier[1].in_features, num_classes)\n",
    "eff = train_model(eff, \"EfficientNet\", head_epochs=5, finetune_epochs=20)\n",
    "\n",
    "conv = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "conv.classifier[2] = nn.Linear(conv.classifier[2].in_features, num_classes)\n",
    "conv = train_model(conv, \"ConvNeXt\", head_epochs=10, finetune_epochs=30)\n",
    "\n",
    "\n",
    "# ---- Evaluate on Test ----\n",
    "def evaluate_model(model, name):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"[{name}] Test Accuracy: {acc:.2f}%\")\n",
    "\n",
    "\n",
    "evaluate_model(resnet, \"ResNet50\")\n",
    "evaluate_model(eff, \"EfficientNet\")\n",
    "evaluate_model(conv, \"ConvNeXt\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
